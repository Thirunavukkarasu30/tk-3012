{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5746cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sabar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>)          │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ gpt2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPT2</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1027</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3027</span>)    │   <span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1027\u001b[0m)          │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ gpt2 (\u001b[38;5;33mGPT2\u001b[0m)                   │ (\u001b[38;5;34m1027\u001b[0m, \u001b[38;5;34m1027\u001b[0m, \u001b[38;5;34m3027\u001b[0m)    │   \u001b[38;5;34m90,497,235\u001b[0m │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> (345.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,497,235\u001b[0m (345.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,497,235</span> (345.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,497,235\u001b[0m (345.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# MultiHead Self Attention\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.att_head_size = embed_dim // num_heads\n",
    "\n",
    "        self.wq = Dense(embed_dim)\n",
    "        self.wk = Dense(embed_dim)\n",
    "        self.wv = Dense(embed_dim)\n",
    "        self.dense = Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.att_head_size))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_w = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_w, v)\n",
    "\n",
    "        output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(output, (batch_size, -1, self.embed_dim))\n",
    "\n",
    "        return self.dense(concat_attention)\n",
    "\n",
    "\n",
    "class FeedForwardNN(Layer):\n",
    "    def __init__(self, embed_dim, dff):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(dff, activation='gelu')\n",
    "        self.dense2 = Dense(embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))\n",
    "\n",
    "\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForwardNN(embed_dim, dff)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        att_output = self.att(x, x, x, mask)\n",
    "        att_output = self.dropout1(att_output)\n",
    "        out1 = self.norm1(x + att_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        return self.norm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class GPT2(Model):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim=768, num_heads=12, dff=3072, num_layers=12, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = Embedding(max_length, embed_dim)\n",
    "\n",
    "        self.transformer_blocks = [Transformer(embed_dim, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.out = Dense(vocab_size)\n",
    "\n",
    "    def create_causal_mask(self, seq_len):\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return 1 - mask  # Converts upper-triangle values to zero\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = self.create_causal_mask(seq_len)\n",
    "\n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb(tf.range(seq_len)[:, tf.newaxis])\n",
    "        x = token_emb + pos_emb\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "# Model Initialization\n",
    "VOCAB_SIZE = 3027\n",
    "MAX_LENGTH = 1027\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32)\n",
    "output = GPT2(vocab_size=VOCAB_SIZE, max_length=MAX_LENGTH)(inputs)\n",
    "gpt2 = Model(inputs, output)\n",
    "\n",
    "gpt2.build(input_shape=(1, MAX_LENGTH))\n",
    "gpt2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e48e0b-c957-4c0f-91b4-645d3850adbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (4.13.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\sabar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.13.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf8275-3196-419b-92aa-5308cce7d689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gpt2_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gpt2_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape          </span>┃<span style=\"font-weight: bold\">      Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304,000</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          │ ?                     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Transformer</span>)  │ ?                     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">7,087,872</span> │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ layer_normalization_49        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)          │                       │              │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3000</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">2,307,000</span> │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)       │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m768\u001b[0m)        │    \u001b[38;5;34m2,304,000\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)          │ ?                     │            \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_12 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_13 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_14 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_15 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_16 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_17 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_18 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_19 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_20 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_21 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_22 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ transformer_23 (\u001b[38;5;33mTransformer\u001b[0m)  │ ?                     │    \u001b[38;5;34m7,087,872\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ layer_normalization_49        │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m768\u001b[0m)        │        \u001b[38;5;34m1,536\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)          │                       │              │\n",
       "├───────────────────────────────┼───────────────────────┼──────────────┤\n",
       "│ dense_121 (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m, \u001b[38;5;34m3000\u001b[0m)       │    \u001b[38;5;34m2,307,000\u001b[0m │\n",
       "└───────────────────────────────┴───────────────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,453,432</span> (345.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,453,432\u001b[0m (345.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,453,432</span> (345.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,453,432\u001b[0m (345.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Wikipedia content for 'Biology'...\n",
      "Generating text with the new model...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import wikipedia\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras import Model\n",
    "import random\n",
    "\n",
    "# Constants with better values for optimization\n",
    "VOCAB_SIZE = 3000\n",
    "MAX_LENGTH = 1024\n",
    "EMBED_DIM = 768\n",
    "NUM_HEADS = 12\n",
    "DFF = 3072\n",
    "NUM_LAYERS = 12\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.qkv_proj = Dense(3 * embed_dim)\n",
    "        self.output_proj = Dense(embed_dim)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        qkv = self.qkv_proj(x)\n",
    "        qkv = tf.reshape(qkv, [batch_size, seq_len, 3, self.num_heads, self.head_dim])\n",
    "        qkv = tf.transpose(qkv, [2, 0, 3, 1, 4])\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        scaled_attention = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_attention = scaled_attention / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention = tf.where(mask == 0, -1e9, scaled_attention)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        output = tf.transpose(output, [0, 2, 1, 3])\n",
    "        output = tf.reshape(output, [batch_size, seq_len, self.embed_dim])\n",
    "\n",
    "        return self.output_proj(output)\n",
    "\n",
    "\n",
    "class FeedForwardNN(Layer):\n",
    "    def __init__(self, embed_dim, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(dff)\n",
    "        self.dense2 = Dense(embed_dim)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense1(x)\n",
    "        x = tf.nn.gelu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForwardNN(embed_dim, dff, dropout_rate)\n",
    "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        normalized_x = self.norm1(x)\n",
    "        att_output = self.att(normalized_x, mask, training=training)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = x + att_output\n",
    "\n",
    "        normalized_out1 = self.norm2(out1)\n",
    "        ffn_output = self.ffn(normalized_out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        return out1 + ffn_output\n",
    "\n",
    "\n",
    "class GPT2(Model):\n",
    "    def __init__(self, vocab_size, max_length, embed_dim=768, num_heads=12, dff=3072, num_layers=12, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.token_emb = Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = self.add_weight(\n",
    "            name=\"positional_embeddings\",\n",
    "            shape=[max_length, embed_dim],\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.transformer_blocks = [\n",
    "            Transformer(embed_dim, num_heads, dff, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.norm = LayerNormalization(epsilon=1e-6)\n",
    "        self.out = Dense(vocab_size)\n",
    "\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((max_length, max_length)), -1, 0)\n",
    "        self.causal_mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "\n",
    "        token_emb = self.token_emb(x)\n",
    "        pos_emb = self.pos_emb[:seq_len]\n",
    "        x = token_emb + pos_emb\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, mask, training=training)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.out(x)\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
    "        input_ids = tf.convert_to_tensor([input_ids], dtype=tf.int32)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            context = input_ids[:, -self.max_length:]\n",
    "            logits = self(context, training=False)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            if top_k > 0:\n",
    "                values, _ = tf.math.top_k(next_token_logits, k=top_k)\n",
    "                min_value = values[:, -1]\n",
    "                next_token_logits = tf.where(\n",
    "                    next_token_logits < min_value[:, tf.newaxis],\n",
    "                    tf.ones_like(next_token_logits) * -1e10,\n",
    "                    next_token_logits\n",
    "                )\n",
    "\n",
    "            probs = tf.nn.softmax(next_token_logits, axis=-1)\n",
    "            next_token = tf.random.categorical(tf.math.log(probs), num_samples=1, dtype=tf.int32)\n",
    "            input_ids = tf.concat([input_ids, next_token], axis=1)\n",
    "\n",
    "        return input_ids[0].numpy()\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size=VOCAB_SIZE):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token2idx = {'': 0, '': 1, '': 2}\n",
    "        self.idx2token = {0: '', 1: '', 2: ''}\n",
    "        self.word_pattern = re.compile(r\"\\b\\w+\\b\")\n",
    "\n",
    "    def fit_on_text(self, text):\n",
    "        text = text.lower()\n",
    "        tokens = self.word_pattern.findall(text)\n",
    "\n",
    "        freq = {}\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "\n",
    "        sorted_tokens = sorted(freq.items(), key=lambda x: -x[1])\n",
    "        vocab_tokens = [t[0] for t in sorted_tokens[:self.vocab_size - 3]]\n",
    "\n",
    "        for i, token in enumerate(vocab_tokens):\n",
    "            idx = i + 3\n",
    "            self.token2idx[token] = idx\n",
    "            self.idx2token[idx] = token\n",
    "\n",
    "        return self\n",
    "\n",
    "    def encode(self, text, max_length=MAX_LENGTH):\n",
    "        text = text.lower()\n",
    "        tokens = self.word_pattern.findall(text)\n",
    "        ids = [self.token2idx.get(t, 1) for t in tokens[:max_length-1]]\n",
    "        ids.append(2)\n",
    "\n",
    "        if len(ids) < max_length:\n",
    "            ids += [0] * (max_length - len(ids))\n",
    "\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = []\n",
    "        for id_ in ids:\n",
    "            if id_ == 0 or id_ == 2:\n",
    "                break\n",
    "            tokens.append(self.idx2token.get(id_, ''))\n",
    "\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def fetch_wiki_text(topic, max_retries=3):\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            page = wikipedia.page(topic, auto_suggest=False)\n",
    "            return page.content\n",
    "        except wikipedia.DisambiguationError as e:\n",
    "            if e.options:\n",
    "                try:\n",
    "                    page = wikipedia.page(e.options[0], auto_suggest=False)\n",
    "                    return page.content\n",
    "                except:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {topic}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return f\"Unable to fetch content for '{topic}' after {max_retries} attempts.\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing GPT-2 model...\")\n",
    "    gpt2 = GPT2(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        max_length=MAX_LENGTH,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dff=DFF,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "\n",
    "    # Build the model with dummy input to enable model.summary()\n",
    "    dummy_input = tf.constant(np.zeros((1, MAX_LENGTH), dtype=np.int32))\n",
    "    gpt2(dummy_input)  # Trigger build\n",
    "    gpt2.summary()\n",
    "\n",
    "    topic = \"Biology\"\n",
    "    print(f\"Fetching Wikipedia content for '{topic}'...\")\n",
    "\n",
    "    text = fetch_wiki_text(topic)\n",
    "    tokenizer = Tokenizer(vocab_size=VOCAB_SIZE).fit_on_text(text)\n",
    "\n",
    "    input_ids = tokenizer.encode(text[:500])\n",
    "\n",
    "    print(\"Generating text with the new model...\")\n",
    "    output_ids = gpt2.generate(input_ids, max_new_tokens=50, temperature=0.8, top_k=40)\n",
    "    generated_text = tokenizer.decode(output_ids)\n",
    "\n",
    "    print(\"=== Generated Output ===\")\n",
    "    print(generated_text[:800])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7dc034-37cd-48ec-8efc-d306c73983e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
